# cnkiPaparDownloader

# 知网万篇级论文下载爬虫

## 简介



一个知网论文爬虫。**初衷**是下载所有某领域的中文文献，进行断句和词频分析，结合自然语言处理、Citespace引文共现网络，辅助研究人员对某领域的研究和论文写作。同时搭建自己的论文查重库，并配合语义相似度算法，实现一个论文查重系统。

该项目为**大规模爬取论文部分**，目标是百万级别的论文爬取。

功能：知网文献总体上常用的部分可以分为学术期刊和学位论文。其中学术期刊可以便捷地下载HTML格式，使用正则表达式直接提取文本。学位论文则需要下载为PDF格式并进行转换。在实践过程中，我对两种文献均进行了实现。此处仅示例学术期刊。



**注**：仅作为个人项目能力展示，仅供学习和测试使用。**为防止代码滥用，源码密码在个人简历里**。

## 执行流程



![1624084884444](https://vincentvor.github.io/imgpages/%E7%9F%A5%E7%BD%91%E4%B8%87%E7%AF%87%E7%BA%A7%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD%E7%88%AC%E8%99%AB.assets/1624084884444.png)

### 1.查询某一关键字下所有的题录信息。

如“比亚迪　战略管理”，搜索到24217条数据。知网系统限制最多访问到前6000条。我们可以根据自己的需要，按照某种标准排序，如**相关度**。

![1624083558180](https://vincentvor.github.io/imgpages/%E7%9F%A5%E7%BD%91%E4%B8%87%E7%AF%87%E7%BA%A7%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD%E7%88%AC%E8%99%AB.assets/1624083558180.png)

### 2.执行题录数据爬取器

利用Fiddler抓包，获取到爬取题录信息所需的数据，复制到题录下载器python文件中并执行。结果保存在doi.db数据库中。（在源代码里可以看到）

### 3.启动下载器

### 4.文献下载完成

利用下载器对知网论文进行下载。从doi.db中爬取题录信息和链接，将结果以文本形式保存在htmllunwen目录中。其中分为多线程和单线程。这里仅对单线程下载器进行了实现。

![1624084042295](https://vincentvor.github.io/imgpages/%E7%9F%A5%E7%BD%91%E4%B8%87%E7%AF%87%E7%BA%A7%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD%E7%88%AC%E8%99%AB.assets/1624084042295.png)



## 问题解决方案

### 验证码解决方案：

知网对爬取题录信息、下载论文均有验证码机制、频繁屏蔽机制等屏蔽爬虫机制。此处专门对其进行研究，构造出5个函数通过反爬虫机制。

对于无法绕过的验证码，有两种方案。

**第一，调用OCR 进行识别。**

**第二，这里使用通知机制+人工处理进行解决**，即发送验证码到管理员（我）的手机，管理员填写验证码发回。此处运用到微信后端开发技术+FLASK服务器搭建，同样在源代码中进行了实现。

还有其他反爬虫措施，均在源代码进行了实现。



### 有效性检验：

这里列举其中一种无效情况：“当前账号并发过多，请稍后下载”。

Python自动判断这句文本是否在页面出现，若出现则执行相应措施。







## 总结

经过4天的开发，本次爬虫功能基本完备。经过实际测试，在48小时共下载了超过20000篇供应链、战略管理等领域的文献，如果增加机器数量、缩短爬取频率，效率还可以更高。

本次测试的意义在于，第一有利于积累论文文本，从而为下一步AI写作、论文分析、论文查重系统构建等项目奠定基础。第二提升了系统构建爬虫的能力。本次应用了微信后端开发技术、FLASK服务器搭建技术、爬虫技术，同时这些代码可以方便地复用，从而有利于日后更多爬虫的开发。

此次测试均使用淘宝购买的知网账号和非校园网服务器，**对本校校园网知网账号无任何影响**。



## 局限性

可以加入多台机器进行爬取，研发大规模分布式爬虫和高弹性、高可用爬虫集群。



## 注意事项

仅供测试，不保证本文所述一切内容真实性，不承担法律责任。
